import gym
import time
import pickle
import sys
import os
import random
from stable_baselines3 import PPO, DQN, A2C
import torch
from torch import nn
from torch.nn import functional as F
from collections import OrderedDict
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
from graphviz import Digraph
from copy import deepcopy

import mujoco_py

sys.path.insert(0,'..')
sys.path.insert(1,'../..')

from envs.HalfCheetah.policy_gradients.torch_utils import ZFilter, Identity

from other_attacks.optimal_attack.policy_gradients.models import CtsPolicy, CtsLSTMPolicy

class ExtendedCheetah():
    def __init__(self, network, custom_env, model):
        self.network = network
        self.batchNetwork = deepcopy(network)
        self.qnetwork = False
        self.modelType = model
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)
        self.custom_env = custom_env
        self.custom_env.state_filter.read_only = True
        self.seed = None
        os.environ["CUBLAS_WORKSPACE_CONFIG"]=":16:8"
        self.recreateActions = []
        
    def reset(self, node):
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)
        
        if type(node) == int:
            self.seed = node
            return self.custom_env.reset(node)

        #print("RECREATING ACTIONS RESET:", node.recreateActions)
        
        if self.modelType == "ATLA":
            self.network.hidden = [torch.zeros(1, 1, self.network.hidden_sizes[1]),
                                   torch.zeros(1, 1, self.network.hidden_sizes[1])]

        self.seed=0
        state = self.custom_env.reset(self.seed)

        if isinstance(node, (np.ndarray, list)):
            return state

        if node == None:
            return state
            
        # For multiple segmented tree searches
        for action in self.recreateActions:
            stateTensor = torch.tensor(state).type(torch.FloatTensor).unsqueeze(0)
            action_pds = self.network(stateTensor)
            state, reward, done, _ = self.custom_env.step(action)
            
        for action in node.recreateActions:
            stateTensor = torch.tensor(state).type(torch.FloatTensor).unsqueeze(0)
            action_pds = self.network(stateTensor)
            state, reward, done, _ = self.custom_env.step(action)
                
        return state

    def _get_obs(self):
        qpos = self.env.sim.data.qpos
        qvel = self.env.sim.data.qvel
        return np.concatenate([qpos, qvel]).ravel()

    def step(self, action):
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)

        obs, reward, _, info = self.custom_env.step(action)

        #done = self.custom_env.env.sim.data.qpos[1] < -0.5
        done = False
        
        return obs, reward, done, info

    def batchExecute(self, batchStates):
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)

        '''
        output = []
        
        for state in batchStates:
            self.batchNetwork.hidden = [torch.clone(self.network.hidden[0]), torch.clone(self.network.hidden[1])]
            state = torch.tensor(state).type(torch.FloatTensor).unsqueeze(0)
            action_pds = self.batchNetwork(torch.tensor(state).type(torch.FloatTensor))
            a = torch.clamp(action_pds[0], min=-1, max=1).detach().numpy()
            output.append(a[0])

        return output
        '''
        action_pds = self.batchNetwork(torch.tensor(batchStates).type(torch.FloatTensor))
        return torch.clamp(action_pds[0], min=-1, max=1).detach().numpy()

    '''
    # Keep Hidden State The Same
    def kh_predict(self, state):
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)
        stateTensor = torch.tensor(state).type(torch.FloatTensor).unsqueeze(0)
        hidden = [torch.clone(self.network.hidden[0]), torch.clone(self.network.hidden[1])]
        action_pds = self.network(stateTensor)
        self.network.hidden = hidden
        return torch.clamp(action_pds[0], min=-1, max=1).detach().numpy()
    '''
    
    def predict(self, state):
        torch.manual_seed(0)
        np.random.seed(0)
        random.seed(0)
        stateTensor = torch.tensor(state).type(torch.FloatTensor).unsqueeze(0)
        action_pds = self.network(stateTensor)
        return torch.clamp(action_pds[0], min=-1, max=1).detach().numpy()


class HalfCheetah:
    def __init__(self, model="PPO"):
        self.bounds = [
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5],
            [-5, 5]
       ]

        if model == "PPO":
            self.checkpoint = torch.load("../envs/HalfCheetah/HalfCheetah_PPO.model")
            #self.checkpoint = torch.load("policy_gradients/HalfCheetah/HalfCheetah_PPO.model")
            self.model = CtsPolicy(17, 6, "orthogonal")
        elif model == "ATLA":
            self.checkpoint = torch.load("../envs/HalfCheetah/HalfCheetah_ATLA.model")
            #self.checkpoint = torch.load("policy_gradients/HalfCheetah/HalfCheetah_ATLA.model")
            self.model = CtsLSTMPolicy(17, 6, "orthogonal")
        else:
            exit("Enter valid model choice(PPO or ATLA)")
            
        #Load model
        self.model.load_state_dict(self.checkpoint['policy_model'])
        self.model.log_stdev.data[:] = -100
        self.model.eval()

        #Load environment
        self.custom_env = self.checkpoint['envs'][0]
        self.env = ExtendedCheetah(self.model, self.custom_env, model)

        self.onnxFilename = None
        self.actionBounds = [[-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1]]
        self.mask = list(range(5,16))
        self.continuous = True
        self.safetyCheck = False
        self.name = "Half Cheetah"
        self.requiresTime = False
        self.usesCustom = True

